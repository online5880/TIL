## Today I Learned (2024-09-04)
---
> ## 목차
- [Today I Learned (2024-09-04)](#today-i-learned-2024-09-04)
- [오늘 공부한 내용](#오늘-공부한-내용)
  - [1. KDT (ML)](#1-kdt-ml)
    - [논리게이트](#논리게이트)
    - [논리게이트 좌표계](#논리게이트-좌표계)
    - [퍼셉트론](#퍼셉트론)
    - [비용 함수](#비용-함수)
    - [활성 함수](#활성-함수)
    - [모델 복잡도와 설계](#모델-복잡도와-설계)
    - [학습 과정 시각화](#학습-과정-시각화)
    - [피처 선택](#피처-선택)
- [어려웠던 내용](#어려웠던-내용)
- [궁금한 내용과 부족한 내용](#궁금한-내용과-부족한-내용)
- [느낀 점](#느낀-점)
---

## 오늘 공부한 내용
### 1. KDT (ML)
#### 논리게이트
| 게이트 | 논리기호 | 논리식 | 진리표 | 연산 |
| --- | --- | --- | --- | --- |
| AND | ∧ | A ∧ B | A와 B 모두 1일 때만 1 | 곱셈 |
| OR | ∨ | A ∨ B | A 또는 B 중 하나가 1이면 1 | 덧셈 |
| NOT | ¬ | ¬A | A가 1이면 0, 0이면 1 | 반전 |
| NAND | ⊼ | ¬(A ∧ B) | A와 B가 모두 1일 때만 0 | AND의 반전 |
| NOR | ⊽ | ¬(A ∨ B) | A 또는 B 중 하나가 1이면 0 | OR의 반전 |
| XOR | ⊕ | A ⊕ B | A와 B가 다를 때 1 | 배타적 덧셈 |
| XNOR | ⊙ | ¬(A ⊕ B) | A와 B가 같을 때 1 | XOR의 반전 |

#### 논리게이트 좌표계
논리게이트는 머신러닝에서 분류 작업에 활용될 수 있으며, 아래의 논리게이트들이 2차원 좌표계에서 선형 분리가 가능한지 여부를 설명할 수 있다:

- **AND**: 선형 분리 가능.
- **OR**: 선형 분리 가능.
- **NAND**: 선형 분리 가능.
- **XOR**: 선형 분리 불가 (비선형 분류에 사용됨)

#### 퍼셉트론
퍼셉트론은 고전적인 인공 신경망 모델로, 다음과 같은 식으로 동작한다.

- **y = Wx + b**
  - **W**: 가중치(Weight)
  - **x**: 입력값
  - **b**: 편향(Bias)

- 퍼셉트론은 간단한 논리 연산(예: AND, OR 등)을 처리할 수 있으며, 단층 퍼셉트론으로는 선형 분류가 가능
- XOR와 같은 비선형 문제를 해결하려면 다층 퍼셉트론이 필요

#### 비용 함수

비용 함수는 모델이 예측한 값과 실제 값 사이의 오차를 측정하는 함수

- MSE (Mean Squared Error)
  - 평균 제곱 오차로, 회귀 문제에서 주로 사용

- RMSE (Root Mean Squared Error)
  - MSE의 제곱근으로, 더 직관적인 오차 척도

- 교차 엔트로피 손실 (Cross Entropy Loss)
  - 분류 문제에서 주로 사용되며, 모델의 예측값이 실제 값과 얼마나 다른지를 계산
  - 이진 분류에서는 **Binary Crossentropy**를, 다중 분류에서는 **Categorical Crossentropy**를 사용

#### 활성 함수

활성 함수는 신경망에서 입력값을 어떻게 변환하여 출력값으로 만들지를 결정하는 역할

- **ReLU (Rectified Linear Unit)**: 0 이하의 값은 0으로 변환하고, 그 이상의 값은 그대로 반환한다.
- **Sigmoid**: 입력값을 0과 1 사이로 변환한다. 주로 이진 분류에서 사용된다.
- **Softmax**: 출력값을 확률 분포로 변환하며, 다중 분류에서 주로 사용된다.
- **GELU/ELU**: 최근에 많이 사용되는 활성 함수로, 모델의 성능을 향상시킨다.

#### 모델 복잡도와 설계
- 모델의 복잡도는 주로 레이어의 수와 각 레이어에 있는 노드의 수에 따라 결정 
- 복잡도가 증가하면 과적합 위험도 커진다.
  - **파라미터 개수**: 레이어를 추가할 때마다 파라미터 수가 증가하며, 모델의 복잡도가 높아진다.
  - **Dropout**: 과적합을 방지하기 위해 일부 노드를 학습 과정에서 무작위로 제외한다.
  - **Output Shape**: 모델의 출력 차원은 작게, 레이어는 많이 구성하는 것이 일반적인 전략이다.

#### 학습 과정 시각화

모델의 학습 과정에서 **loss**와 **accuracy**를 시각화하여 학습이 잘 이루어지고 있는지 확인할 수 있다.

- **Loss 그래프**: 기울기가 완만하게 하강하는 것이 이상적이다.
- **Accuracy 그래프**: 로그 형태로 증가하는 그래프가 정상적인 학습을 나타낸다.
  
<img src="https://github.com/online5880/TIL/blob/main/Images/2024.09/2024_09_04/loss_accu.png?raw=true" width="100%" height="100%"/>

#### 피처 선택
- 모델의 성능을 높이기 위해서는 **피처 개수**를 줄이는 것이 중요
- 불필요한 피처는 제거하고, 중요한 피처만을 선택하여 모델의 복잡도를 낮추는 것이 좋다.
---
## 어려웠던 내용
- 딥러닝
---
## 궁금한 내용과 부족한 내용
- 딥러닝.. 어떻게 잘 써먹을까
---
## 느낀 점
- 쉽지 않겠다.

<!-- <img src="이미지 주소" width="100%" height="100%"/> -->
