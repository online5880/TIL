## Today I Learned (2024-09-26)
---
> ## 목차
- [Today I Learned (2024-09-26)](#today-i-learned-2024-09-26)
- [오늘 공부한 내용](#오늘-공부한-내용)
  - [1. KDT(NLP)](#1-kdtnlp)
    - [TF-IDF](#tf-idf)
  - [2. 딥 러닝을 이용한 자연어 처리 입문(NLP)](#2-딥-러닝을-이용한-자연어-처리-입문nlp)
    - [토큰화(Tokenization)](#토큰화tokenization)
    - [정제(Cleaning)와 정규화(Normalization)](#정제cleaning와-정규화normalization)
- [어려웠던 내용](#어려웠던-내용)
- [궁금한 내용과 부족한 내용](#궁금한-내용과-부족한-내용)
- [느낀 점](#느낀-점)
---

## 오늘 공부한 내용
### 1. KDT(NLP)
#### TF-IDF
- TF (Term Frequency, 단어 빈도) : 어떤 단어가 문서 내에서 얼마나 자주 등장하는지를 나타내는 지표
- DF (Document Frequency, 문서 빈도) : 어떤 단어가 문서**군** 내에서 얼마나 자주 등장하는지를 나타내는 지표. 이 때, 등장 빈도는 해당 단어가 존재하는지 여부만 체크합니다
- IDF (Inverse Document Frequency, 역문서 빈도) : 문서 빈도의 역수
- TF-IDF : TF * IDF
- 어떤 단어 $t$ 가 있고, 문서 $d$ 가 있다 $d$ 를 포함한 문서군은 $D$ 이다. 이 때, 아래와 같이 정의한다.
  - 불린 빈도 $tf(t,d)$ = $t$ 가 $d$ 에 나타났는가? True: 1, False: 0
  - $f(t,d)$ : 문서 내에서 단어의 총 빈도

### 2. 딥 러닝을 이용한 자연어 처리 입문(NLP)
#### 토큰화(Tokenization)
- 단어 토큰화(Word Tokenization)
  - 토큰의 기종늘 단어(word)로 하는 경우, 단어 토큰화(word tokenization)라고 한다.
  - 여기서 단어(word)는 단어 단위 외에도 단어구, 의미를 갖는 문자열로도 간주되기도 한다.
- 토큰화에서 고려해야할 사항
  1. 구두점이나 특수 문자를 단순 제외해서는 안된다.
      - ex) AT&T, $54.55, 01/02/06(날짜)
  2. 줄임말과 단어 내에 띄어쓰기가 있는 경우
       - ex) what’re, we’re, i’m

- 문장 토큰화(Sentence Tokenization)
  - 코퍼스 내에서 문장 단위로 구분하는 작업
  - 때로는 문장 분류(sentence segmentation)라고도 부른다.
  - ex) IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 aaa@gmail.com로 결과 좀 보내줘. 그 후 점심 먹으러 가자.
  - 문장 토큰화 라이브러리
      - NLTK - sent_tokenize(영어)
      - kss(한국어)
  - 형태소(morpheme)
      - 뜻을 가진 가장 작은 말의 단위
      - 자립 형태소 :
          - 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소.
          - 그 자체로 단어가 된다.
          - 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등
      - 의존 형태소 :
          - 다른 형태소와 결합하여 사용되는 형태소
          - 접사, 어미, 조사, 어간을 말한다.

#### 정제(Cleaning)와 정규화(Normalization)

- 정제(Cleaning)
  - **목적**: 코퍼스에서 **노이즈 데이터**를 제거하는 작업입니다.
  - **작업 시점**: 토큰화 이전이나 이후에 진행될 수 있습니다.
  - **노이즈 데이터**: 의미 없는 글자들(특수 문자 등) 또는 분석 목적에 맞지 않는 단어들.

- 정규화(Normalization)
  - **목적**: 표현 방법이 다른 단어들을 **동일한 형태**로 통합하는 작업입니다.
  - 예) **USA**와 **US**처럼 동일한 의미의 단어를 하나로 통합.
- 주요 정제 및 정규화 기법
  - 규칙 기반 표기 통합
  - **같은 의미를 가진 단어**라도 다른 형태로 표현되는 경우가 있습니다. 이를 **규칙**을 통해 통합합니다.
  - 예) **USA**와 **US**, **uh-huh**와 **uhhuh**를 같은 단어로 처리.

- 대/소문자 통합
  - **영어의 대문자와 소문자**는 의미가 동일하기 때문에 대부분 소문자로 변환합니다.
  - 예) **Automobile** → **automobile**.
  - 단, 특정 상황에서는 대문자와 소문자를 구분해야 합니다. 예) **US**(미국)와 **us**(우리).
- 불필요한 단어 제거
  - **불용어** 및 **길이가 짧은 단어**, **등장 빈도가 매우 적은 단어** 등을 제거합니다.
  - 예) 불용어 제거, 등장 빈도가 5번 이하인 단어 제거, 길이가 1~2자인 단어 제거.
- 정규 표현식(Regular Expression)
  - 코퍼스에서 **패턴**을 잡아내어 노이즈를 제거하는 방법입니다.
  - 예) HTML 태그, 날짜 등의 불필요한 정보를 정규 표현식을 사용하여 제거.
- 기타 유의사항
  - **영어와 한국어의 차이**:
      - 영어 단어는 평균적으로 6~7자, 한국어 단어는 2~3자로 추정됩니다.
      - **영어**에서는 길이가 짧은 단어가 대부분 불용어이지만, **한국어**는 두 글자 단어도 의미가 있어 같은 기준을 적용하기 어렵습니다.
- 예시 코드: 불필요한 단어 제거 (길이가 1~2인 단어 제거)

  ```python
  import re
  text = "I was wondering if anyone out there could enlighten me on this car."

  # 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제
  shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')
  print(shortword.sub('', text))
  # 결과: "was wondering anyone out there could enlighten this car."

  ```

---
## 어려웠던 내용
- tf-idf에 대한 개념
---
## 궁금한 내용과 부족한 내용
- tf-idf를 이용한 것들?
---
## 느낀 점
- 어렵다 어려워 

<!-- <img src="이미지 주소" width="100%" height="100%"/> -->
