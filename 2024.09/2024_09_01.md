## Today I Learned (2024-09-01)
---
> ## 목차
- [Today I Learned (2024-09-01)](#today-i-learned-2024-09-01)
- [오늘 공부한 내용](#오늘-공부한-내용)
  - [1. 머신러닝](#1-머신러닝)
    - [K-Nearest Neighbor(KNN)](#k-nearest-neighborknn)
    - [Logistic Regression(로지스틱 회귀)](#logistic-regression로지스틱-회귀)
    - [SVM(Support Vector Machine) / SVC](#svmsupport-vector-machine--svc)
    - [Decision Tree(의사결정나무)](#decision-tree의사결정나무)
    - [Ensemble(앙상블)](#ensemble앙상블)
    - [편향(Bias) 와 분산(Variance)](#편향bias-와-분산variance)
    - [Ensemble과 Bagging, Boosting](#ensemble과-bagging-boosting)
    - [Bagging : Random Forest](#bagging--random-forest)
    - [Bagging : Extra Tree](#bagging--extra-tree)
  - [2. 메타코드(Pandas와 Bigquery를 활용한 데이터 분석)](#2-메타코드pandas와-bigquery를-활용한-데이터-분석)
    - [Pandas](#pandas)
- [어려웠던 내용](#어려웠던-내용)
- [궁금한 내용과 부족한 내용](#궁금한-내용과-부족한-내용)
- [느낀 점](#느낀-점)
---

## 오늘 공부한 내용
### 1. 머신러닝
#### K-Nearest Neighbor(KNN)

<img src=" https://github.com/online5880/TIL/blob/main/Images/2024.09/2024_09_01/KNN.png?raw=true" width="50%" height="100%"/>


- 기본적인 머신러닝 기법으로 실제로는 많이 사용되지 않지만, 가볍게 이해해둬야 하는 기법
- k 값(거리)을 설정한 후, k 거리 안에 들어온 기존 데이터(훈련 데이터)의 타입(분류)이 다수인쪽으로 예측하는 기법
- 장점 : 단순하고 이해하기 쉬움
- 단점 : 적절한 k 값 설정에 어려움이 있고, 데이터와 feature 수가 많아지면, 예측 시간이 오래걸림
- 군집화에 더 많이 쓰이는 기법

#### Logistic Regression(로지스틱 회귀)

<img src=" https://github.com/online5880/TIL/blob/main/Images/2024.09/2024_09_01/LR.png?raw=true" width="50%" height="100%"/>

- 이름에 Regression 이 들어가지만, 회귀 모델이 아닌 분류 모델이다.
- Linear Classification 과 달리, “S”자 모양으로 곡선을 계산하는 시그모이드 함수(Sigmoid function)을 사용해서 분류하는 모델
- 로지스틱 회귀는 정확히 0 또는 1을 예측하는 대신 확률(0과 1 사이의 값)을 생성해서 0과 1을 분류하는 예측 모델

#### SVM(Support Vector Machine) / SVC

<img src=" https://github.com/online5880/TIL/blob/main/Images/2024.09/2024_09_01/SVC.png?raw=true" width="50%" height="100%"/>

- SVM(Support Vector Machine)은 분류를 위해, 분류의 경계가 되는 경계선을 작성하여, 분류를 실행하는 모델
    - margin : 결정선과 가장 가까운 양 옆의 데이터 간의 거리
    - 결정선과 가장 가까운 양 옆의 데이터를 Support Vector 라고함
    - SVM 은 margin 을 최대화하는 결정선(Decision Boundary)을 구하는 알고리즘
- sklearn 라이브러리에서는 SVM을 SVC라는 SVM의 마지막 글자를 Classification 으로 바꾼 명청으로, 관련 함수를 제공함

#### Decision Tree(의사결정나무)

<img src=" https://github.com/online5880/TIL/blob/main/Images/2024.09/2024_09_01/DecsionTree.png?raw=true" width="50%" height="100%"/>

- 의사결정나무(Decision Tree)라고도 함
- feature 와 데이터를 분석하여 패턴을 찾아내고, 예측 가능한 규칙들을 Yes/No 의 항목으로 나무(Tree)처럼 조합해서 예측하는 기법

#### Ensemble(앙상블)

<img src=" https://github.com/online5880/TIL/blob/main/Images/2024.09/2024_09_01/Ensemble.png?raw=true" width="50%" height="100%"/> [출처](https://medium.com/@thedatabeast/all-about-ensemble-learning-35158c82d713)

- 앙상블(Ensemble) 기법은 머신러닝에서 실제로 가장 많이 사용되는 기법
- 앙상블 기법에도 여러 기법이 있지만,
    - 기본적인 컨셉은 여러 개의 기본 모델을 생성한 후, 전체 모델의 예측 값을 종합하여 하나의 최종 모델 또는 예측값을 도출하는 방법

#### 편향(Bias) 와 분산(Variance)

<img src=" https://github.com/online5880/TIL/blob/main/Images/2024.09/2024_09_01/Bias_Variance.png?raw=true" width="50%" height="100%"/>

- 빨간색 부분이 모델의 목표 구역(실제값)이며, 파란색 점이 예측값
- 편향(Bias)이 높다면, 학습 데이터에 대해서도 예측 정확도가 떨어지는 경우를 의미
    - 과소적합(Underfitting)일 수 있고, 지나치게 단순한 직선(또는 모델)을 사용한 경우일 수 있다.
- 분산(Variance)이 크다면, 학습 데이터에 대한 예측 정확도와 달리, 새로운 데이터셋에 대해서는 정확도가 확연히 차이나는 경우를 의미함.
    - 과적합(Overfitting)일 수 있고, 지나치게 복잡한 곡선(또는 모델)을 사용한 경우일 수 있다.
- 분산이 높고, 편향이 낮은 경우
    - 과적합이 되고, 조금만 데이터가 달라도 다른 예측치를 내놓는 경우
- 분산이 낮고, 편향이 높은 경우
    - 과소적합이 되어, 대부분 데이터가 잘못된 구역에 틀린 예측치를 놓는 경우

#### Ensemble과 Bagging, Boosting

<img src=" https://github.com/online5880/TIL/blob/main/Images/2024.09/2024_09_01/Bagging_Boosting.png?raw=true" width="50%" height="100%"/>

- **Bagging**
  - Bagging은 Bootstrap Aggregating의 줄임말
  - 통계학에서 부트스트랩(BootStrap)이란
      - 주어신 샘플데이터셋에서, 중복을 허용하여 무작위로 임의 샘플셋을 복수개로 만들고 이를 통해, 분포, 신뢰구간과 같은 통계값을 계산하여, 모집단 또는 주어진 샘플 데이터셋보다 큰 데이터에 대한 분포를 추정하는 기법
  - Bagging은 학습 데이터에서 샘플 데이터를 여러 번 bootstrap 방식으로 추출
      - 각 모델을 학습시킨 후, 각 모델의 결과를 합쳐서 최종 예측을 만드는 방식
      - 동일 데이터에서 여러 샘플 데이터를 만들 수 있으므로, 학습 데이터 수를 늘리는 효과가 있음

#### Bagging : Random Forest

<img src=" https://github.com/online5880/TIL/blob/main/Images/2024.09/2024_09_01/Random_Forest.png?raw=true" width="50%" height="100%"/> [출처](https://incodom.kr/Random_Forest)

- 어떤 데이터든 예측 성능이 중간(또는 그 이상)은 간다고 하는 기법
- bootstrap 방식으로 학습 데이터에서 여러 개의 샘플 데이터 추출
- 전체 feature 중 일부 feature 만 랜덤하게 선택하여 여러 개의 의사결정트리를 만든다.
    - 예를 들어, 전체 feature가 50개이면, 각 의사결정트리에서 사용한 feature 수를 20개로 정한 후, 50 개 중 20 개의 feature 를 랜덤 선택
- 각 tree들을 각 샘플 데이터로 학습하여 각 tree의 조건을 샘플 데이터에 적합하게 만든다.
- 테스트 데이터로 각 tree 별 예측 수행
- 각 tree 의 예측값을 투표(aggregation)을 통해 다수결로 최종 예측값을 결정하는 방법

#### Bagging : Extra Tree
- 익스트림 랜덤 트리(extremely randomized trees) 또는 엑스트라 트리(Extra Trees)라고 부름
- 랜덤 포레스트 보다 훨씬 더 랜덤한 성격을 가짐
    - 샘플 데이터 수까지도 랜덤하게 결정해서, 다양한 모델을 만들고, 분류 조건도 랜덤하게 분할하여, 무작위성을 증대시키고, 이를 통해 최종 모델을 만듬

### 2. 메타코드([Pandas와 Bigquery를 활용한 데이터 분석](https://www.metacodes.co.kr/edu/read2.nx?M2_IDX=31635&EP_IDX=12191&EM_IDX=12015))
#### Pandas
- 데이터 오브젝트 생성(DataFrame)
- 데이터 확인하기
  - head, tail, describe
- 데이터 선택하기
  - loc, iloc
- 결측 데이터
  - isna, fillna
- merge/grouping/reshaping
  - concat, merge
  - groupby
  - pivot_table

---
## 어려웠던 내용
- 다양한 머신러닝 모델들?
---
## 궁금한 내용과 부족한 내용
- 머신러닝
---
## 느낀 점
- 역시 실습이 재미있다.

<!-- <img src="이미지 주소" width="100%" height="100%"/> -->
