## Today I Learned (2024-10-01)
---
> ## 목차
- [Today I Learned (2024-10-01)](#today-i-learned-2024-10-01)
- [오늘 공부한 내용](#오늘-공부한-내용)
  - [1. 머신러닝](#1-머신러닝)
    - [RMSLE(Root Mean Square Log Error) 함수 만들기](#rmsleroot-mean-square-log-error-함수-만들기)
    - [회귀 모델 사용법 익히기](#회귀-모델-사용법-익히기)
    - [모델 블렌딩 전략으로 캐글 제출하기](#모델-블렌딩-전략으로-캐글-제출하기)
  - [2. 토스 데이터분석 test](#2-토스-데이터분석-test)
    - [SQL Query 테스트](#sql-query-테스트)
- [어려웠던 내용](#어려웠던-내용)
- [궁금한 내용과 부족한 내용](#궁금한-내용과-부족한-내용)
- [느낀 점](#느낀-점)
---

## 오늘 공부한 내용
### 1. 머신러닝
#### RMSLE(Root Mean Square Log Error) 함수 만들기
- 캐글의 평가지표가 RMSLE 이고, sklearn에 rmsle 가 존재하지 않음
  
```python
def get_rmsle(y_actual, y_pred):
    # 음수 값을 방지하기 위해, 최소한 0 이상의 값을 갖도록 함
    y_pred = np.maximum(0, y_pred)
    # 로그를 취하기 전에, 값이 0 이상임을 보장
    log_pred = np.log(np.maximum(0, y_pred) + 1)
    log_actual = np.log(np.maximum(0, y_actual) + 1)
    diff = log_pred - log_actual
    mean_error = np.square(diff).mean()
    return np.sqrt(mean_error)
```

#### 회귀 모델 사용법 익히기
- Linear Regression
  - 선형 회귀(Linear Regression)는 독립 변수와 종속 변수 간의 선형 관계를 모델링하는 가장 기본적인 회귀 방법임.

    ```python
    from sklearn.linear_model import LinearRegression

    # 데이터 준비
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # 모델 생성 및 학습
    model = LinearRegression()
    model.fit(X_train, y_train)
    ```
- Lasso Regression
  - Lasso(Least Absolute Shrinkage and Selection Operator) 회귀는 특성 선택을 수행하며, 특성 수를 줄여 과적합을 방지함.
  - alpha: L1 규제 강도 (0에 가까울수록 규제가 약함, 클수록 규제가 강함).
  - max_iter: 최적화 알고리즘의 최대 반복 횟수 (수렴 속도와 관련됨).

    ```python
    # 모델 생성 및 학습
    from sklearn.linear_model import Lasso
    from sklearn.model_selection import GridSearchCV

    hyperparams = {'max_iter': [1000, 1500, 2000, 2500, 3000], 
                  'alpha': 1/np.array([0.1, 1, 2, 3, 4, 10, 30,100,200,300,400,800,900,1000])
    }

    lasso_grid=GridSearchCV(estimator = Lasso(), param_grid = hyperparams, 
                    verbose=True, scoring=rmsle_scorer, cv=5, n_jobs=-1)

    lasso_grid.fit(X_train, y_train)
    ```

- Ridge Regression
  - Ridge 회귀는 L2 정규화를 사용하여 모델을 규제하며, 모든 변수들이 일부 영향력을 가지게 함으로써 과적합을 방지함.
  - alpha : L2 규제 강도를 조절하고, 모델의 복잡도를 제어
  - max_iter: 최적화 알고리즘의 최대 반복 횟수 (수렴 속도와 관련됨).

    ```python
    # 모델 생성 및 학습
    from sklearn.linear_model import Ridge
    from sklearn.model_selection import GridSearchCV

    hyperparams = {'max_iter': [1000, 1500, 2000, 2500, 3000], 
                  'alpha':[0.1, 1, 2, 3, 4, 10, 30,100,200,300,400,800,900,1000]
    }

    ridge_grid=GridSearchCV(estimator = Ridge(), param_grid = hyperparams, 
                    verbose=True, scoring=rmsle_scorer, cv=5, n_jobs=-1)

    ridge_grid.fit(X_train, y_train)
    ```

- Random Forest Regression
  - 랜덤 포레스트 회귀(Random Forest Regression)는 여러 개의 결정 트리를 사용하여 예측의 평균을 구함으로써 높은 예측 성능을 제공함.

    ```python
    from sklearn.ensemble import RandomForestRegressor

    # 모델 생성 및 학습
    model = RandomForestRegressor(n_estimators=100)
    model.fit(X_train, y_train)
    ```

- XGBoost Regression
  - XGBoost는 강력한 성능을 제공하는 부스팅 알고리즘 중 하나로, 대규모 데이터셋에서도 효율적으로 동작함.

    ```python
    import xgboost as xgb

    # 모델 생성 및 학습
    model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)
    model.fit(X_train, y_train)
    ```
- Gradient Boosting Regression
  - Gradient Boosting Regression은 모델의 성능을 개선하기 위해 잔차를 기반으로 모델을 반복적으로 학습시키는 방식임.

    ```python
    from sklearn.ensemble import GradientBoostingRegressor

    # 모델 생성 및 학습
    model = GradientBoostingRegressor(n_estimators=100)
    model.fit(X_train, y_train)
    ```

#### 모델 블렌딩 전략으로 캐글 제출하기
- XGBoost + Random Forest 
- Gradient Boost + Random Forest
  - 해당 모델이 가장 좋음

### 2. 토스 데이터분석 test
#### SQL Query 테스트
- 난이도가 생각보다는 높지 않다고 느꼈지만,
- 많이 풀지는 못했다... 껄껄
---
## 어려웠던 내용
- 다양한 머신러닝 모델들
- 아직은 어렵다
- 토스 테스트
---
## 궁금한 내용과 부족한 내용
- 없음
---
## 느낀 점
- SQL 공부 열심히 하자
- 10월도 화이팅

<!-- <img src="이미지 주소" width="100%" height="100%"/> -->
