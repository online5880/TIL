## Today I Learned (2024-10-11)
---
> ## 목차
- [Today I Learned (2024-10-11)](#today-i-learned-2024-10-11)
- [오늘 공부한 내용](#오늘-공부한-내용)
  - [1. KDT (NLP)](#1-kdt-nlp)
    - [자연어 처리 딥러닝 모델 변천사 및 개념 정리](#자연어-처리-딥러닝-모델-변천사-및-개념-정리)
  - [2. ADsP](#2-adsp)
    - [3과목 끝내기](#3과목-끝내기)
- [어려웠던 내용](#어려웠던-내용)
- [궁금한 내용과 부족한 내용](#궁금한-내용과-부족한-내용)
- [느낀 점](#느낀-점)
---

## 오늘 공부한 내용
### 1. KDT (NLP)
#### 자연어 처리 딥러닝 모델 변천사 및 개념 정리
- 딥러닝 모델 변천사
  - **LSTM**
  - **CNN + LSTM**
  - **Seq2Seq**
  - **Transformer**
  - **BERT**
  - **GPT**

- **Embedding Vector**
  - 단어의 의미를 다차원 공간에 벡터화하는 방법.
  - 분산 표현(Distributed Representation)을 통해 단어 간 의미적 유사성을 벡터화함.
    - **Word2Vec**: CBOW, Skip-gram 방식으로 학습.
    - **CBOW**: 주변 단어로 중심 단어 예측.
    - **Skip-gram**: 중심 단어로 주변 단어 예측.
    - **원-핫 벡터 vs 임베딩 벡터**
      - 원-핫 벡터: 고차원, 희소 벡터, 수동으로 표현.
      - 임베딩 벡터: 저차원, 밀집 벡터, 훈련 데이터로부터 학습함.
      - 예시:
        - 원-핫 벡터: `강아지 = [ 0 0 0 0 1 0 0 0 0 0 0 0 ... 중략 ... 0]`
        - 임베딩 벡터: `강아지 = [0.2 1.8 1.1 -2.1 1.1 2.8 ... 중략 ...]`

- **Transformer 모델 구조**
  - Encoder-Decoder 구조:
    - Encoder: 여러 개의 Encoder Layer로 구성. 이전 레이어의 출력이 다음 레이어의 입력으로 사용됨.
      - **Sub layer1**: Multi-Head Attention (self)
      - **Sub layer2**: Feed Forward
    - Decoder: 여러 개의 Decoder Layer로 구성. 이전 레이어의 출력이 다음 레이어의 입력으로 사용됨.
      - **Sub layer1**: Masked Multi-Head Attention (Self)
      - **Sub layer2**: Multi-Head Attention
      - **Sub layer3**: Feed Forward
  - **Positional Encoding**:
    - 입력 시퀀스의 순서를 고려하기 위한 방법.

- **Attention 메커니즘**
  - **Self-Attention**:
    - 입력 문장 내 단어들 간의 유사도를 계산해, 문맥 정보를 반영함.
    - 예: '그것(it)'이 '동물(animal)'과 연관되었을 확률을 계산함.

  - **Multi-Head Attention**:
    - 다양한 Feature를 추출하기 위해 여러 개의 Attention을 병렬적으로 사용.
    - **Self Attention**: Query, Key, Value가 동일함.

  - **Scaled Dot-Production Attention**:
    - Attention 메커니즘에서 Query와 Key의 내적을 계산하여 중요도를 측정함.

- **LLM 학습 원리**
  - **LLM Input/Output**:
    - 텍스트를 입력으로 받아 토큰 단위로 분할 후, 토큰 인덱스와 매핑하여 학습.
    - 출력 토큰은 '다음 토큰'에 대한 확률 분포를 예측함.
    - 동일한 입력이라도 매번 동일한 출력을 얻지 못할 수 있음.

  - **Transformer Parameter**:
    - Batch_size, N_head, D_hidn, D_head, N_seq 등의 파라미터 설명.

### 2. ADsP
#### 3과목 끝내기
- 1회차 끝...
---
## 어려웠던 내용
- 딱히 없었다.
---
## 궁금한 내용과 부족한 내용
- ADsP 3과목 이해하고 외우기
---
## 느낀 점
- 그래도 배웠던 내용들이 좀 있어서 다행이였다.

<!-- <img src="이미지 주소" width="100%" height="100%"/> -->
