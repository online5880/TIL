## Today I Learned (2024-07-12)
---
> ## 목차
- [Today I Learned (2024-07-12)](#today-i-learned-2024-07-12)
- [오늘 공부한 내용](#오늘-공부한-내용)
  - [1. KDT(Machine Learning)](#1-kdtmachine-learning)
    - [Classification(분류)](#classification분류)
    - [t-value(유의수준)](#t-value유의수준)
    - [결정 트리(Decision Tree)](#결정-트리decision-tree)
    - [KNN(K-Nearest Neighbor) 알고리즘](#knnk-nearest-neighbor-알고리즘)
    - [유클리드 거리(Euclidean Distance)](#유클리드-거리euclidean-distance)
    - [앙상블 학습(모델)](#앙상블-학습모델)
    - [회귀(Regression)](#회귀regression)
    - [경사하강법(Gradient Descent)](#경사하강법gradient-descent)
- [어려웠던 내용](#어려웠던-내용)
- [궁금한 내용과 부족한 내용](#궁금한-내용과-부족한-내용)
- [느낀 점](#느낀-점)
---

## 오늘 공부한 내용
### 1. KDT(Machine Learning)
#### Classification(분류)
- `분류`는 학습데이터로 주어진 데이터의 피쳐와 레이블값을 머신러닝 알고리즘으로 학습해 모델을 생성하고, 이렇게 생성된 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블 값을 예측하는 것
- **분류의 종류**
  - 이진 분류(Binary Classification)
    - 입력된 데이터를 두 그룹(참 혹은 거짓)으로 분류하는 것
    - **이진 분류 성능 평가 지표**
      - `Confusion Matrix(오차행렬)` : 레이블별 예측 결과를 정리한 행렬
        - <img src="https://github.com/online5880/TIL/blob/main/Images/2024.08/2024_08_20/Confusion_Matrix.png?raw=true" width="100%" height="100%"/> 
        - 출처 : [이진 분류의 성능 평가 지표: Accuracy, Confusion Matrix, Precision, Recall, F1, ROC-AUC](https://driip.me/3ef36050-f5a3-41ea-9f23-874afe665342)
      - `정밀도(Precision) / 재현율(Recall)`
        - 정밀도(Precision) : $\frac{TP}{TP + FP} = \frac{\text{진짜 양성}}{\text{진짜 양성} + \text{가짜 양성}}$ = 양성이라 예측한 것 중 진짜 양성
        - 재현율(Recall) : $\frac{TP}{TP + FN} = \frac{\text{진짜 양성}}{\text{진짜 양성} + \text{가짜 음성}}$ = 진짜 양성 중 양성이라 잘 예측한 비율
        - 모델 사용 목적에 따라 Precision과 Recall의 중요도가 다를 수 있다.
      - `F1 Score`
        - 정밀도(Precision)와 재현율(Recall)의 조화 평균 F1 점수를 이용하여 분류기의 성능을 평가하기도 함
        - $F_1 = \frac{2}{\frac{1}{정밀도}\frac{1}{재현율}}$
      - `ROC 곡선`
        - 이진 분류기의 성능을 측정할 수 있다.
        - 곡선 읽는법
          - 대각선 : 쓸모없다.
          - 윗곡선 : 완벽한 그래프
          - 아랫곡선 : 완벽하게 반대로 찍는다.
            - 하지만 *-1을 하면 뒤집을 수 있기 때문에 좋은 곡선이다.
  - 다중 분류(Multiclass Classification)
    - 세개 이상의 클래스로 데이터를 분루하여, 다항 분류라고도 불린다.

#### t-value(유의수준)
- a, b 그룹에서 서로 다른 수준인가를 검증하기 위한 것.
- 0.05미만이면 해당 가설이 유의하다. (95% 확률)
  - 업계, 데이터, 상황에 따라 달라진다.

#### 결정 트리(Decision Tree)
- 매우 직관적이다.
- 데이터의 특징(Feature)에 따라 조건문(if-else) 형태로 데이터를 분류하는 과정
- 사람이 이해하기 쉽고 간단하게 설명할 수 있는 특성이 있다.
- <img src="https://github.com/online5880/TIL/blob/main/Images/2024.08/2024_08_20/Decision_Tree.png?raw=true" width="100%" height="100%"/> 
- 출처 : [scikit-learn](https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html)
- **주요 개념 및 하이퍼파라미터**
  - 규제 파라미터(하이퍼파라미터):
    - max_depth: 트리의 최대 깊이.
    - min_sample_split: 노드를 분할하기 위한 최소 샘플 수.
    - min_sample_leaf: 리프 노드에 있어야 하는 최소 샘플 수.
    - min_weight_fraction_leaf: 리프 노드에 있는 샘플들의 가중치 합의 최소값.
    - max_leaf_nodes: 최대 리프 노드의 수.
    - max_features: 각 노드 분할에 사용할 최대 특징 수.
  - 지니 불순도 (Gini impurity):
    - 노드의 불순도를 측정하는 지표.
    - 0에 가까울수록 해당 노드에 속한 샘플들이 동일한 클래스에 속함을 의미.
  - samples: 해당 노드에 속한 샘플 수.
  - value: 각 클래스별 샘플 수.
  - class: 가장 높은 비율을 차지하는 클래스로 해당 노드의 예측 클래스.
- **장점과 단점**
  - 장점
    - 이해하고 사용하기 쉽고 직관적이다.
    - 데이터 전처리의 영향을 크게 받지 않는다.
  - 단점
    - 과적합(Overfitting)의 위험이 있으며, 이를 방지하기 위한 규제가 필요하다.

#### KNN(K-Nearest Neighbor) 알고리즘
- `KNN`은 주변 데이터(Neighbor)를 살펴본 뒤 더 많은 데이터가 포함되어 있는 클래스로 분류하는 방식
- K의 개수만큼 주변의 데이터를 살펴본다는 뜻.
- <img src="https://github.com/online5880/TIL/blob/main/Images/2024.08/2024_08_20/KNN_Algorithm.png?raw=true" width="100%" height="100%"/> 

#### 유클리드 거리(Euclidean Distance)
- 두 점 사이의 직선 거리를 계산하는 방법으로, KNN과 같은 알고리즘에서 자주 사용된다.

#### 앙상블 학습(모델)
- 앙상블 학습은 여러 개의 모델을 결합하여 **편향(Bias)**과 **분산(Variance)**을 줄이는 것이 핵심이다.
  - 편향(Bias): 예측값과 실제 값의 차이로 인해 발생하며, 편향이 크면 과소적합(Underfitting)이 발생합니다.
  - 분산(Variance): 입력 데이터의 작은 변동에 반응하는 정도로, 분산이 크면 과대적합(Overfitting)이 발생합니다.
  - <img src="https://github.com/online5880/TIL/blob/main/Images/2024.08/2024_08_20/Ensemble.png?raw=true" width="100%" height="100%"/> 
  
- 주요 기법:
    - <img src="https://github.com/online5880/TIL/blob/main/Images/2024.08/2024_08_20/ensemble_category.png?raw=true" width="100%" height="100%"/> 
  - 보팅(Voting): 여러 분류기의 예측을 투표를 통해 결정.
    - **소프트 보팅(Soft Voting)**과 **하드 보팅(Hard Voting)**이 있음.
  - 배깅(Bagging): 데이터 샘플링 후 각 모델의 예측 결과를 집계.
    - <img src="https://github.com/online5880/TIL/blob/main/Images/2024.08/2024_08_20/Bagging.png?raw=true" width="100%" height="100%"/> 
  - 부스팅(Boosting): 순차적으로 학습하는 다수의 분류기를 결합.
    - 에이다 부스트(AdaBoost): 잘못 예측된 샘플에 더 큰 가중치를 부여해 다시 학습.
    - 그래디언트 부스팅(Gradient Boosting): 이전 모델의 예측 오차를 보정하는 새로운 예측기를 학습.
    -  스태킹(Stacking): 여러 예측기의 결과를 새로운 학습 데이터로 사용하여 예측.
  
#### 회귀(Regression)
- 회귀는 여러 개의 독립변수와 한 개의 종속변수 간의 상관관계를 모델링하는 기법
- Y = $W_n*X_n$
  - w : 가중치
- 선형 회귀(Linear Regression)
  - 단순선형회귀 : y = ax+b 형태의 직선
  - 평균 제곱 오차(Mean Squared Error, MSE)
    - 예측값과 실제값 사이의 오차를 제곱하여 평균을 구한 값
    - 값이 작을수록 좋은 모델
  - 루트 평균 제곱 오차(Root Mean Squared Error, RMSE)
    - MSE에 루트를 씌운 값이다.
    - 0에 가까울수록 좋은 모델을 의미

#### 경사하강법(Gradient Descent)
- 경사하강법은 학습 과정에서 가중치 파라미터를 조금씩 조정하여 비용함수를 최소화하는 방법이다.
  - 배치 크기 (Batch Size): 한 번의 파라미터 업데이트에 사용되는 학습 데이터의 개수.
  - Local Minima와 Global Minima: 학습 과정에서 비용함수를 최소화할 때의 두 가지 가능한 상태.
---
## 어려웠던 내용
- 에브리띵
---
## 궁금한 내용과 부족한 내용
- 에브리띵... Math Algebra Calculus
---
## 느낀 점
- 양이 너무 많다.

<!-- <img src="이미지 주소" width="100%" height="100%"/> -->
