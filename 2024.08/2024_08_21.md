## Today I Learned (2024-08-21)
---
> ## 목차
- [Today I Learned (2024-08-21)](#today-i-learned-2024-08-21)
- [오늘 공부한 내용](#오늘-공부한-내용)
  - [1. KDT(ML)](#1-kdtml)
    - [경사하강법의 종류](#경사하강법의-종류)
    - [다항 회귀](#다항-회귀)
    - [로지스틱 회귀](#로지스틱-회귀)
    - [소프트 맥스 회귀](#소프트-맥스-회귀)
    - [차원의 저주](#차원의-저주)
    - [차원축소](#차원축소)
    - [비지도학습(Unsupervised Learning)](#비지도학습unsupervised-learning)
    - [군집화 (Clustering)](#군집화-clustering)
  - [2. Calculus](#2-calculus)
    - [Limits(극한)](#limits극한)
- [어려웠던 내용](#어려웠던-내용)
- [궁금한 내용과 부족한 내용](#궁금한-내용과-부족한-내용)
- [느낀 점](#느낀-점)
---

## 오늘 공부한 내용
### 1. KDT(ML)
#### 경사하강법의 종류
- 배치 경사하강법
  - 배치크기(Batch size)가 전체 학습데이터셋 크기와 같은, 즉 스텝이 1번 발생하는 경사하강법
  - 특징
    - 학습데이터 셋이 크면 많은 시간과 메모리 필요하다는 단점 존재
  - <img src="https://github.com/online5880/TIL/blob/main/Images/2024.08/2024_08_21/Gradient_descent.png?raw=true" width="50%" height="100%"/> 
- 확률적 경사하강법(SGD)
  - 한번의 스텝에 하나의 데이터에 대한 예측값을 실행한 후에 그 결과를 이용해서 `그래디언트를 계산하고 파라미터를 조정`한다.
  - 참고 사이트 : [확률적 경사 하강법(Stochastic Gradient Descent)](https://velog.io/@dbs991013/%ED%99%95%EB%A5%A0%EC%A0%81-%EA%B2%BD%EC%82%AC-%ED%95%98%EA%B0%95%EB%B2%95Stochastic-Gradient-Descent)
  - <img src="https://github.com/online5880/TIL/blob/main/Images/2024.08/2024_08_21/Stochastic_GD.png?raw=true" width="50%" height="100%"/>
- 미니배치 경차하강법
  - 배츠크기가 2부터 수백사이로 정해지고, 최적의 배치크기는 경우에 따라 모두 다르다.
  - 배치 경사하강법보다 빠르게 학습한다.
  - <img src="https://github.com/online5880/TIL/blob/main/Images/2024.08/2024_08_21/mini_batch.png?raw=true" width="50%" height="100%"/>
- Learning schedule(rate)
  - 학습 스케줄이란?
    - 큰 학습률에서 시작하고 학습속도가 느려질떄 학습률을 낮추면 최적의 고정 학습률보다 더 좋은 솔루션을 빨리 발견할 수 있다.
    - 파라미터를 얼마나 업데이트할 것인지를 결정하는 것이 바로 learning rate(학습률, step size)이다.
    - <img src="https://github.com/online5880/TIL/blob/main/Images/2024.08/2024_08_21/learning_rate.png?raw=true" width="50%" height="100%"/>
#### 다항 회귀
- 선형모델을 사용하여 비선형 데이터를 학습하는 방법
  - 기존 데이터에 특성을 제곱하여 특성을 확장한다.
  - <img src="https://github.com/online5880/TIL/blob/main/Images/2024.08/2024_08_21/polynomial_regression.png?raw=true" width="50%" height="100%"/>

#### 로지스틱 회귀
- 선형회귀 모델이 예측한 값에 시그모이드(Sigmoid)함수를 적용하여 0과 1사이의 값, 즉 양성일 확률로 지정한다.
- `이진 분류 문제`를 해결하는 데 주로 사용되는 통계적 모델
- 참고 사이트 : [로지스틱 회귀(Logistic Regression)](https://velog.io/@skkumin/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1-%ED%9A%8C%EA%B7%80Logistic-Regression)
- <img src="https://github.com/online5880/TIL/blob/main/Images/2024.08/2024_08_21/sigmoid.png?raw=true" width="50%" height="100%"/>

#### 소프트 맥스 회귀
- 다중 클래스 분류
- 여러 클래스로 분류할 때 사용되며, 각 클래스에 속할 확률을 계산한다.

#### 차원의 저주
- 차원이 증가할 수록 데이터 간 빈 공간이 생기게 됨으로써 생기는 문제들을 차원이 저주라고함.
- 연산 시간 증가
- 과적합 위험 커짐
- 차원이 커질수록 임의의 두 지점 사이의 평균 거리가 매무 멀어진다.

#### 차원축소
- 정보의 손실이 크지 않은 방향으로 고차원의 데이터를 저차원 데이터로 변환시키는 것
- 고차원 데이터 -> 저차원 데이터
- 차원축소 기법
  - **사영기법**
    - n차원의 데이터셋을 차원이 낮은 d차원 데이터셋으로 `사영(Projection)`하는 기법
    - <img src="https://github.com/online5880/TIL/blob/main/Images/2024.08/2024_08_21/projection.png?raw=true" width="50%" height="100%"/>
  - PCA(주성분 분석)
    - 가장 인기 있는 차원축소 알고리즘
    - 데이터에 가장 가까운 초평면을정의한 다음, 데이터를 이 평면에 투영
    - 분산 보존
      - 저차원으로 사영할 때 훈련셋의 분산이 최대한 유지되도록 축을 지정해야 한다.
      - <img src="https://github.com/online5880/TIL/blob/main/Images/2024.08/2024_08_21/pca_01.png?raw=true" width="50%" height="100%"/>
  - LDA(선형 판별 분석)
    - 클래스 간 분리를 최대화하는 축을 찾는 기법
  
#### 비지도학습(Unsupervised Learning)
- 레이블 없는 데이터 학습: 
  - 클러스터링, 이상치 탐지, 데이터 밀도 추정 등의 기법을 포함

#### 군집화 (Clustering)
- K-means: 가장 널리 사용되는 군집화 알고리즘으로, `실루엣 계수`를 통해 군집화를 평가할 수 있다.
  - 실수렛 계수란?
    - 각 데이터 포인트와 주위 데이터 포인트들과의 거리 계산을 통해 값을 구하며, 군집 안에 있는 데이터들은 잘 모여있는지, 군집끼리는 서로 잘 구분되는지 클러스터링을 평가하는 지표
    - 같은 그룹내에서 각 점과 다른 점 사이의 거리 평균을 a
    - 어떤그룹에서 다른그룹에서 선을 그엇을 때 평균값을 b
    - 1 일때가 제일 좋다. ↔ 0 일ㅌㅈ₩때 가장 안좋다.
    - 단순히 실루엣 계수만 볼것이 아니라 다이어그램도 보고 선택을 하자
- 계층적 클러스터링: 데이터를 계층적으로 클러스터링하며, 클러스터의 개수를 미리 지정할 필요가 없다.
- DBSCAN: 이상치를 효과적으로 처리하며, 군집 간 밀집도가 다를 때도 사용할 수 있다.
- Gaussian Mixture Model (GMM): 데이터의 분포가 혼합된 가우시안 분포로 모델링되는 경우에 사용된다.

---
### 2. Calculus
#### Limits(극한)
- $\displaystyle{\lim_{x \to 1}}f(x) = 2$
  - x가 1에 가까워지면 함수값은 2에 수렴한다.
  - 상태를 표현하는 것이 극한이다.
---
## 어려웠던 내용
- every thing
---
## 궁금한 내용과 부족한 내용
- every thing
---
## 느낀 점
- 솔직히 너무 많은 내용을 배운다... 허허
- 주말에 다시 정리해야겠다.

<!-- <img src="이미지 주소" width="100%" height="100%"/> -->
